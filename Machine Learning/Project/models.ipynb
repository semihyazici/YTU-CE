{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff510fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/semihyazici/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/semihyazici/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction, model_selection, pipeline, manifold, preprocessing,feature_selection \n",
    "from colorama import Fore, Back, Style\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from colorama import Fore, Back, Style\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "489e7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/semihyazici/Desktop/Semih/Code_Resources/NLP-Proje/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63110fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(PATH,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7af1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633d548d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101069/2843459197.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  neutral['class'] = 1\n"
     ]
    }
   ],
   "source": [
    "neutral = data[(data[\"toxic\"] != 1) & (data[\"severe_toxic\"] != 1)& (data[\"obscene\"] != 1)\n",
    "    & (data[\"threat\"] != 1)& (data[\"insult\"] != 1)& (data[\"identity_hate\"] != 1)]\n",
    "neutral['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f2e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101069/4147647893.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  negative['class'] = 0\n"
     ]
    }
   ],
   "source": [
    "negative = data[(data[\"toxic\"] != 0) | (data[\"severe_toxic\"] != 0)| (data[\"obscene\"] != 0)\n",
    "    | (data[\"threat\"] != 0)| (data[\"insult\"] != 0)| (data[\"identity_hate\"] != 0)]\n",
    "negative['class'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be815caa",
   "metadata": {},
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ffb3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e7fd58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = negative.shape[0]\n",
    "neutral = neutral.sample(sample_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf26c4",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af1b5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c00f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textCleaner(df = None , src = 'comment_text' ,dst = 'text_clean',stop_words = 'english'):\n",
    "    \n",
    "    if df is None:\n",
    "        raise TypeError(\"Data Frame cannot be type 'None'\")\n",
    "    \n",
    "    try:\n",
    "        lst_stopwords = nltk.corpus.stopwords.words(stop_words)\n",
    "    except:\n",
    "        raise Exception( \"'\" + stop_words +\"'\"+\" is not a valid type.\")\n",
    "    df[dst] = df[src].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f970c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101069/2085000868.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[dst] = df[src].apply(lambda x:\n",
      "/home/semihyazici/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "neutral.reset_index(drop = True, inplace = True)\n",
    "textCleaner(df = neutral,stop_words = 'english',src='comment_text')\n",
    "textCleaner(df = negative,stop_words = 'english',src='comment_text')\n",
    "\n",
    "col2remove =  ['id','toxic','severe_toxic',\n",
    "               'obscene','threat','insult','identity_hate','comment_text']\n",
    "neutral.drop(col2remove, inplace = True, axis = 1) \n",
    "negative.drop(col2remove, inplace = True, axis = 1) \n",
    "negative.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23dde189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = np.array(negative['text_clean'].append(neutral['text_clean'])) , np.array(negative['class'].append(neutral['class']))\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000,ngram_range=(1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d71731",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_data,y_data,test_size=0.1,shuffle=True,)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_val,y_val,test_size=0.5,shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "971cfd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semihyazici/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "corpus = X_train\n",
    "vectorizer.fit(corpus)\n",
    "x_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_\n",
    "X_names = vectorizer.get_feature_names()\n",
    "p_value_limit = 0.95\n",
    "dtf_features = pd.DataFrame()\n",
    "for cat in np.unique(y_train):\n",
    "    chi2, p = feature_selection.chi2(x_train, y_train==cat)\n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n",
    "vectorizer.fit(corpus)\n",
    "X_tr = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb33826",
   "metadata": {},
   "source": [
    "# Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "65ce825c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_tr, (y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6eeb5844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8887861667522684"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train başarısı\n",
    "sum(model.predict_proba(X_tr).argmax(axis=1) == y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "54c19f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.876078914919852"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validation Başarısı\n",
    "sum(model.predict_proba(vectorizer.transform(X_val)).argmax(axis=1) == y_val) / len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9ec5c751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8798521256931608"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test başarısı\n",
    "sum(model.predict_proba(vectorizer.transform(X_test)).argmax(axis=1) == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "853439f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[721, 108],\n",
       "       [ 87, 707]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Karışıklık matrisi\n",
    "confusion_matrix(model.predict_proba(vectorizer.transform(X_test)).argmax(axis=1),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51e1bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prob_words(txt,vectorizer,model):\n",
    "    tokens = vectorizer.transform(txt)\n",
    "    token_nums = vectorizer.inverse_transform(tokens)[0]\n",
    "    ltl_map = model.feature_log_prob_\n",
    "    vocab = vectorizer.vocabulary\n",
    "    word_map = {token_nums[i]:ltl_map[:,token].argmax()   for i,token in enumerate(tokens.indices)}\n",
    "    return [word  for word,val in word_map.items() if val == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0461f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordHighlight():\n",
    "    \n",
    "    def __init__(self,highlight = True):\n",
    "        \n",
    "        self.highlight = highlight\n",
    "        \n",
    "    def __call__(self,original_txt,res):    \n",
    "        color = Back.RED\n",
    "        out_sent = ''\n",
    "        \n",
    "        if type(original_txt) != str:\n",
    "            original_txt = str(original_txt).split()\n",
    "        \n",
    "        \n",
    "        for word in original_txt:\n",
    "\n",
    "            checker = sum([1 if resp_word in word else 0 for resp_word in res])\n",
    "            if checker:\n",
    "                if self.highlight:\n",
    "                    out_sent += color+  word + Style.RESET_ALL+' '\n",
    "                else:\n",
    "                    out_sent += '*** '\n",
    "                    \n",
    "            else:\n",
    "                out_sent += word + ' '\n",
    "\n",
    "        return out_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "65b23463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i \u001b[41mdont\u001b[0m know who that \u001b[41mbastard\u001b[0m was but he will pay with blood i'll \u001b[41mkill\u001b[0m him \n"
     ]
    }
   ],
   "source": [
    "sentence = np.array([\"i dont know who that bastard was but he will pay with blood i'll kill him\"])\n",
    "df = pd.DataFrame({\"class\":\"0\",\"comment_text\":sentence})\n",
    "textCleaner(df = df,stop_words = 'english',src='comment_text')\n",
    "tokens = vectorizer.transform(df.text_clean)\n",
    "\n",
    "res = find_prob_words(df.text_clean,vectorizer=vectorizer,model=model)\n",
    "\n",
    "a = WordHighlight(highlight=True)\n",
    "print(a(sentence[0],res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb3067",
   "metadata": {},
   "source": [
    "# With RNN/LSTM/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5ba542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe42c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(pretrained)\n",
    "maxlen = 150\n",
    "limit = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d13ec5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexes = []\n",
    "for i,text in enumerate(X_val):\n",
    "    if len(text.split()) > limit:\n",
    "        indexes.append(i)\n",
    "X_val = np.delete(X_val,indexes)\n",
    "y_val = np.delete(y_val,indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1d6566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for i,text in enumerate(X_test):\n",
    "    if len(text.split()) > limit:\n",
    "        indexes.append(i)\n",
    "X_test = np.delete(X_test,indexes)\n",
    "y_test = np.delete(y_test,indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1015e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for i,text in enumerate(corpus):\n",
    "    if len(text.split()) > limit:\n",
    "        indexes.append(i)\n",
    "corpus = np.delete(corpus,indexes)\n",
    "y_train = np.delete(y_train,indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d85a63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    def __init__(self,tokenizer,sentences,labels,maxlen,device ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.maxlen = maxlen\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        return self.tokenizer(self.sentences[index],max_length=self.maxlen,padding='max_length',\n",
    "                              return_tensors=\"pt\").to(device),torch.tensor(self.labels[index],\n",
    "                                                                                        device = device)\n",
    "    def __len__(self,):\n",
    "        return len(self.sentences)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60dac4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_vocab,d_model,maxlen,bidirectional = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.times = (2 if bidirectional  else 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab,self.d_model,)\n",
    "        self.rnn = nn.RNN(self.d_model,self.d_model,1,batch_first = True,\n",
    "                          bidirectional = bidirectional)\n",
    "        self.attention =  nn.Linear(self.d_model * self.times,1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.times * d_model,256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(.15),\n",
    "            nn.ReLU()    ,\n",
    "            nn.Linear(256,128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(.15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,2))\n",
    "    \n",
    "    \n",
    "    def _generate_initial_hidden(self,batch_size):\n",
    "        return torch.zeros((self.times,batch_size,self.d_model ),device = device,\n",
    "                           dtype=torch.float32)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        embeds = self.embedding(x)\n",
    "        batch_size = embeds.shape[0]\n",
    "        initial_hidden = self._generate_initial_hidden(batch_size)\n",
    "        h,c = self.rnn(embeds,initial_hidden)\n",
    "        mask = mask.permute(0,2,1)\n",
    "        attentions = self.softmax(self.attention(h * mask))\n",
    "        h = attentions * h\n",
    "        outs = self.classifier(h[:,-1,:])\n",
    "        #outs = self.classifier(torch.rand((batch_size,h.shape[-1])))\n",
    "        return outs,attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4853c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_vocab,d_model,maxlen,bidirectional = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.times = (2 if bidirectional  else 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab,self.d_model,)\n",
    "        self.gru = nn.GRU(self.d_model,self.d_model,1,batch_first = True,\n",
    "                          bidirectional = bidirectional)\n",
    "        self.attention =  nn.Linear(self.d_model * self.times,1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.times * d_model,256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(.15),\n",
    "            nn.ReLU()    ,\n",
    "            nn.Linear(256,128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(.15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,2))\n",
    "    \n",
    "    \n",
    "    def _generate_initial_hidden(self,batch_size):\n",
    "        return torch.zeros((self.times,batch_size,self.d_model ),device = device,\n",
    "                           dtype=torch.float32)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        embeds = self.embedding(x)\n",
    "        batch_size = embeds.shape[0]\n",
    "        initial_hidden = self._generate_initial_hidden(batch_size)\n",
    "        h,c = self.gru(embeds,initial_hidden)\n",
    "        mask = mask.permute(0,2,1)\n",
    "        attentions = self.softmax(self.attention(h * mask))\n",
    "        h = attentions * h\n",
    "        outs = self.classifier(h[:,-1,:])\n",
    "        #outs = self.classifier(torch.rand((batch_size,h.shape[-1])))\n",
    "        return outs,attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6614c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_vocab,d_model,maxlen,bidirectional = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.times = (2 if bidirectional  else 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab,self.d_model,)\n",
    "        self.lstm = nn.LSTM(self.d_model,self.d_model,1,batch_first = True,\n",
    "                          bidirectional = bidirectional)\n",
    "        self.attention =  nn.Linear(self.d_model * self.times,1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.times * d_model,256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(.15),\n",
    "            nn.ReLU()    ,\n",
    "            nn.Linear(256,128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(.15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,2))\n",
    "    \n",
    "    \n",
    "    def _generate_initial_hidden(self,batch_size):\n",
    "        return (torch.zeros((self.times,batch_size,self.d_model ),device = device,\n",
    "                           dtype=torch.float32),\n",
    "                torch.zeros((self.times,batch_size,self.d_model ),device = device,\n",
    "                           dtype=torch.float32))\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        embeds = self.embedding(x)\n",
    "        batch_size = embeds.shape[0]\n",
    "        h0,c0 = self._generate_initial_hidden(batch_size)\n",
    "        h,c = self.lstm(embeds,(h0,c0))\n",
    "        mask = mask.permute(0,2,1)\n",
    "        attentions = self.softmax(self.attention(h * mask))\n",
    "        h = attentions * h\n",
    "        outs = self.classifier(h[:,-1,:])\n",
    "        #outs = self.classifier(torch.rand((batch_size,h.shape[-1])))\n",
    "        return outs,attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c163d799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(n_vocab = tokenizer.vocab_size,\n",
    "              d_model= 256,\n",
    "              maxlen = maxlen,\n",
    "              bidirectional = True).to(device)\n",
    "model.load_state_dict(torch.load(\"rnn_params.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a692f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUModel(n_vocab = tokenizer.vocab_size,\n",
    "              d_model= 256,\n",
    "              maxlen = maxlen,\n",
    "              bidirectional = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f98bbac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(n_vocab = tokenizer.vocab_size,\n",
    "              d_model= 256,\n",
    "              maxlen = maxlen,\n",
    "              bidirectional = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d13c86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "n_iter = 1000\n",
    "print_per_iter = 20#n_iter // 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5146508b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(30522, 256)\n",
       "  (rnn): RNN(256, 256, batch_first=True, bidirectional=True)\n",
       "  (attention): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Dropout(p=0.15, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): Dropout(p=0.15, inplace=False)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(),lr = lr,)\n",
    "data = Dataset(tokenizer,corpus,y_train,maxlen,device)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size,shuffle=True) \n",
    "\n",
    "data_val = Dataset(tokenizer,X_val,y_val,maxlen,device)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset=data_val,batch_size=batch_size,shuffle=True) \n",
    "data_test = Dataset(tokenizer,X_test,y_test,maxlen,device)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=data_test,batch_size=batch_size,shuffle=True) \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "570bbf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:45<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 Loss = 357.7144889831543 Train Accuracy = 0.8345937728881836 Val Accuracy = 0.8259783387184143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:44<00:00, 22.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 Loss = 189.9455473460257 Train Accuracy = 0.9329375624656677 Val Accuracy = 0.8459616899490356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:52<00:00, 19.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 Loss = 125.1020605545491 Train Accuracy = 0.9587187767028809 Val Accuracy = 0.8376352787017822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                        | 24/1000 [00:02<01:29, 10.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_161048/480362423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_161048/1708953207.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         return self.tokenizer(self.sentences[index],max_length=self.maxlen,padding='max_length',\n\u001b[0m\u001b[1;32m     11\u001b[0m                               \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2483\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2484\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2590\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2591\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2663\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2664\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFFD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_161048/480362423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in (range(n_epochs)):\n",
    "    model.train()\n",
    "    \n",
    "    cum_loss = 0.0\n",
    "    train_acc = 0\n",
    "    for iteration in tqdm(range(n_iter)):\n",
    "        try:\n",
    "\n",
    "            batch,labels =  next(iter(dataloader))\n",
    "            sentences,masks = batch['input_ids'],batch['attention_mask']\n",
    "        except KeyboardInterrupt:\n",
    "            raise KeyboardInterrupt()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        optim.zero_grad()\n",
    "\n",
    "        preds,_ = model(sentences.squeeze(1),masks)\n",
    "        \n",
    "        loss = loss_fn(preds,labels)\n",
    "        train_acc += torch.sum(preds.argmax(dim=1) == labels)\n",
    "        cum_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    \n",
    "    \n",
    "    final_train_acc = train_acc/(n_iter * batch_size)\n",
    "    \n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch,labels in dataloader_val:\n",
    "        sentences,masks = batch['input_ids'],batch['attention_mask']\n",
    "        with torch.no_grad():\n",
    "            preds,_ = model(sentences.squeeze(1),masks)\n",
    "        test_acc += torch.sum(preds.argmax(dim=1) == labels)\n",
    "    test_acc = test_acc/len(data_test)\n",
    "    \n",
    "    print(f\"Epoch = {epoch} Loss = {cum_loss} Train Accuracy = {final_train_acc} Val Accuracy = {test_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f4a919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8843, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "preds_mem = []\n",
    "true_mem = []\n",
    "model.eval()\n",
    "    \n",
    "for batch,labels in dataloader_test:\n",
    "    sentences,masks = batch['input_ids'],batch['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        preds,_ = model(sentences.squeeze(1),masks)\n",
    "        preds_mem.append(preds.argmax(dim=1))\n",
    "    test_acc += torch.sum(preds.argmax(dim=1) == labels)\n",
    "    true_mem.append(labels)\n",
    "test_acc = test_acc/len(data_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "005775e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention2word(tokenizer,tokens,attention,top = 5):\n",
    "    items = attention[0,:,0].topk(top).indices.detach().cpu()\n",
    "    resp_words = [tokenizer.ids_to_tokens[tokens[0][item].detach().cpu().item()] for item in items]\n",
    "    return resp_words\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cc30399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(text):\n",
    "    sentence = np.array([text])\n",
    "\n",
    "    df = pd.DataFrame({\"class\":\"0\",\"comment_text\":sentence})\n",
    "    textCleaner(df = df,stop_words = 'english',src='comment_text')\n",
    "\n",
    "    sent = df.text_clean[0]\n",
    "\n",
    "    batch_test = tokenizer(sent,max_length=maxlen,padding='max_length',\n",
    "                                  return_tensors=\"pt\").to(device)\n",
    "\n",
    "    tokens,mask = batch_test['input_ids'],batch_test['attention_mask'].unsqueeze(0)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred,att = model(tokens,mask)\n",
    "    if pred.detach().cpu().argmax(dim=1) == 1:\n",
    "        print(text)\n",
    "        return\n",
    "    tokens = tokens.detach().cpu()\n",
    "    resp_words = attention2word(tokenizer,tokens,att,top = 3)\n",
    "    a = WordHighlight(highlight=True)\n",
    "    print(a(sentence[0],resp_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fd1bae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont know who that \u001b[41mbastard\u001b[0m was but he will pay with \u001b[41mblood\u001b[0m i'll \u001b[41mkill\u001b[0m him \n"
     ]
    }
   ],
   "source": [
    "test(\"i dont know who that bastard was but he will pay with blood i'll kill him\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "666acec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i think every life has a value \n"
     ]
    }
   ],
   "source": [
    "test(\"i think every life has a value \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9acc36c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That litte \u001b[41mbastard\u001b[0m is trying to \u001b[41mrob\u001b[0m the \u001b[41mbank\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "test(\"That litte bastard is trying to rob the bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdb0e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That litte pimp is trying to rob the bank\n"
     ]
    }
   ],
   "source": [
    "test(\"That litte pimp is trying to rob the bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65af1851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[590,  73],\n",
       "       [ 57, 465]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(torch.concat(preds_mem,dim=0).detach().cpu(),torch.concat(true_mem,dim=0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dba12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba1ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dcd832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f523904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
